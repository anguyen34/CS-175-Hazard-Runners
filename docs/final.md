---
layout:	default
title:	Final Report
---

## Video:

## Project Summary:
For our project, we conducted research of various parameters across three different machine learning algorithms and Monte Carlo search, all of which were used as functional approximators for reinforcement Q-learning. We studied how changing the values of the selected parameters would impact the success of each policy in simulated games against other policies with the same algorithm, but using parameters we defined as the baseline parameters. The platform used for conducting our research was the Tron environment from ColosseumRL, a multiagent reinforcement learning environment comprising several different games. The game we chose, Tron as implemented in ColosseumRL is the Tron light cycles subgame, which is a variation of the snake game concept, but here the goal is to force opponents into walls or trails, while simultaneously avoiding them. However, unlike the original Tron light cycles subgame which was a player versus three AI, the Tron environment we used was four AI playing against each other. 

Our primary goal was to study the effect of changing various parameters of our reinforcement Q-learning functional approximators on our learning rates and overall success in the game for each policy. The three machine learning algorithms used as functional approximators for the Q-learning Greedy Epsilon policy are Random Forest, Neural Network, Monte Carlo Search, and Ensemble which contains a MultiLayer Perceptron Network, Adaboost Regressor, Support Vector Machine, and K-Nearest Neighbor Regressor. To satisfy our primary goal we researched the performance of each functional approximator while changing parameters such as number of hidden layers and nodes per layer for the Neural Network or maximum number of leaves and maximum depth for the Random Forest. Evaluating the performance of a model for each parameter configuration gives insight into the most impactful parameters on the model’s gameplay which is exactly what we are researching.

Our secondary goal was to identify which parameters of high impact were the most beneficial to our AI and to have our four functional approximators with their optimal parameters face off each other. Beyond just observing the impacts of changing certain parameters in a particular functional approximator, we also wanted to observe how the functional approximators compared with each other. We expected our neural network approximator to perform the best, since while researching for functional approximators to use for our project, we learned that neural networks are the most commonly used approximators for reinforcement learning, compared to an approximator like Random Forest. By comparing our neural network approximator with our other ones, we wanted to see if this conclusion also applied to AI that were training on and playing Tron.

What motivated our idea behind the project was the idea that games are a traditional field that AI are used in, such as Tron originally using AI as opponents. Developing better AI could result in more challenging and interesting AI opponents for Tron and other games. Machine learning is a core part of AI, since it can help produce solutions to problems where the solutions are hard to describe. An instance of this is Tron, where the solution to the game is winning against four other players. A solution must produce a series of actions for the player in which they are the winner, but also account for the actions of other players, and the numerous potential board states. Determining machine learning parameters that could result in a faster learning AI or a more successful AI are beneficial to actually producing better AI within a quicker time frame. Through researching the various machine learning parameters across several algorithms, we hoped to find some of the parameters that could lead to faster learning or more successful AI.


## Approaches:
Our approach for our research is to change values for certain parameters we are interested in for each of our four agents in order to examine the impact these parameters have on the performance of our agents in Tron. Our four agents are Greedy Epsilon Reinforcement Q Learning policies that use the following function approximators: Neural Network, Ensemble, Random Forest, and Monte Carlo Search. For each agent we chose a set of parameters that we hypothesized to have a substantial impact on the agent’s performance as well as values for each parameter that were interesting in researching. We then measured the performance of the agents with these different parameter values playing against agents of the same type but with baseline parameter values which allows us to study the impact of these different parameters.

To implement our Neural Network agent, we utilized the Deep Q Network algorithm from the Rays Rlib library. This agent begins with initializing the Tron environment and Ray’s Rlib by defining the general config as well as the policy/Neural Network structure. We use 4 Neural Networks (1 for each player) where the 0th player is the agent with changing parameters, while agents 1-3 have baseline parameters and all agents utilize a Proximal Policy Optimization algorithm which is a type of actor critic optimization that attempts to minimize the loss according to this function: 
![](images/loss.png?raw=true) 
We then run the test function which will run the game of Tron with all 4 players continuously for the specified number of epochs which we chose to be 100 for this agent (meaning 100 games) because this agent takes a large amount of time to run. At the beginning of a game we train the agent using the Deep Q Network which is trained on the position of each player’s head and the game board observation that is preprocessed as follows: the board is rotated such that the current player appears as if they are player 0, there is a border on the board, and every other player appears as the same value rather than their player number. While the ith game is running, Ray’s Rlib computes an action to take for each player depending on the current state, the previous action, and previous reward. This action is then passed to the step function where a greedy epsilon policy is used in order to choose and use a final action on the state for that player as well as calculate the cumulative reward for that player for the purposes of data collection. This function returns the new state, reward, and a boolean representing if the game has terminated. This happens for the duration of the game then the whole algorithm repeats for the next game and so forth for all i games in the number of epochs.
 
For the data collection of the Neural Network agent, we chose to change the number of hidden layers, number of nodes per layer, activation function, and the value of epsilon for our greedy epsilon policy. For hidden layers we chose to test values of 1, 2, 5, 10, 20 with the default/baseline value used for the dummy agents being 1; we didn’t test values higher than 20 for performance/time reasons. Before testing we hypothesized that lower values for this parameter would result in underfitting while higher values might result in overfitting as well as taking more time to train. For the  number of nodes per layer we tested values of 1, 2, 5, 10, 25, 50, 64 with the baseline value being 64. Before testing we believed that, similar to the number of hidden layers, less nodes would underfit the data while taking shorter time to train while larger values might overfit the data and take longer to train. For the activation function, we tested ReLU, Swish, and TanH with ReLU being the baseline value. We believed that ReLU would be the best value for this as we found that it is typically more commonly used than the others, otherwise there aren’t many advantages/disadvantages to these values as this parameter is more of something that you test to see which is best rather than having preset best values. For epsilon, we chose values of 0.01, 0.05, 0.1, 0.25, 0.5 with the baseline being 0.01. Increasing this value increases the volatility of the data as the randomness is increased, however this may be beneficial by letting the regressor learn new patterns it would not have discovered on its own.

The Ensemble agent was implemented using SKLearn’s Voting Regressor, for which we decided to include an AdaBoost regressor, a K-Nearest Neighbors regressor, a Support Vector Machine regressor, and a MultiLayer Perceptron regressor. Each of the algorithms used as part of the Voting Regressor Ensemble are also from SKLearn. Like with the Neural Network, the Ensemble agent with modified parameters was player 0 and the other three Ensemble agents occupied players 1, 2, and 3. Also like with the Neural Network, each epoch corresponded to one game of Tron which ran until terminal, afterwards the cumulative reward of player 0 and the difference in reward between player 0 and the averaged reward of the other three players was collected for that epoch. In each step of an epoch, each of the living players would have a move chosen for them by their Voting Regressor, the actions of the players would be used to generate the next game state, and then for each player their move, reward, and the board state would be recorded as data. At the end of a game each player would receive a new Voting Regressor to replace their old one, with the new one trained using all of their accumulated data thus far. The data that is used to train each player’s Voting Regressor uses the move taken, the preprocessed board state like in the Neural Network, the current position, deaths, winners, and the positions of other players, where these are all features to predict the cumulative reward. 
 
The parameters chosen to be modified for the Ensemble agent are some of parameters from each of its algorithms, with each parameter being tested in isolation from one another. From the AdaBoost regressor the number of estimators and the loss function were tested. For the Support Vector Machine regressor we chose to alter the kernel function. From the MultiLayer Perceptron regressor we tested changes to the activation function and the number of nodes per hidden layer. Finally, we also tested changes to the value of epsilon. Each parameter value was tested for 100 epochs, like the Neural Network, since the Ensemble took longer to run relative to the Random Forest and Monte Carlo search due to the Multilayer Perceptron. For AdaBoost’s number of estimators we tested the values 10, 25, 50, 100, 150, and 200, with the baseline being 50. We hypothesized that adjusting the number of estimators could alter the success of the model or change whether the model overfits or underfits the collected data. The loss functions tested for AdaBoost were linear, square, and exponential loss functions, with linear being the baseline. The kernel functions we tested for the Support Vector Machine were the linear, polynomial, RBF, and exponential, with RBF being the baseline value. For the Multilayer Perceptron we tested the identity, logistic, ReLU, and TanH activation functions, where ReLU was the baseline value. In changing the loss, kernel, and activation functions we hypothesized that the respective baseline values of linear, RBF, and ReLU would perform the best. The number of nodes tested for the Multilayer Perceptron was 1, 2, 5, 10, 25, 50, 100, and 200, with the baseline being 1 layer and each layer having 100 nodes. We hypothesized that increasing the number of nodes would be advantageous in that it could produce a better model, but could also be disadvantageous by overfitting the data. We further hypothesized that decreasing the number of codes could reduce overfitting if it was occurring, but at the potential of then underfitting the data. Like withThe values of epsilon tested with 0.01, 0.05, 0.10, 0.25, and 0.50, with 0.01 as the baseline. We hypothesized that an advantage of increasing the epsilon value could be that there is a higher likelihood of the agent learning a new beneficial move, but a disadvantage could be that the agent is more likely to take an action that could lead to its death. However, we also did not expect any individual parameter to have a drastic effect on the success of the overall algorithm, as each parameter only affects one algorithm amongst many in the Voting Regressor Ensemble. Instead, we looked to see if any one parameter may alter the learning rate or have any effects at all on the overall Voting Regressor.

Our Random Forest agent was implemented by using SKLearn’s library for a Random Forest Regressor. The implementation of this agent is the exact same as the Ensemble Agent with the only differences being that this agent utilizes a Random Forest Regressor rather than a Voting Regressor Ensemble and uses different parameters for training/data collection.
The parameters we decided to change for our Random Forest were the number of estimators used, the max depth, the max leaf nodes and the epsilon value used for greedy exploration. These parameter changes were done in isolation. Each parameter value was tested for 200 epochs, since the Random Forest ran relatively quickly, so we could afford to test for more epochs. We tested the number of estimators of 10, 25, 50, 100, 150, and 200 and the baseline value was 100. We hypothesized that 200 trees could possibly lead to more accurate predictions. The values we tested for max depth were 2, 5, 10, 25, 50, 100, 200 and None which was the baseline value. Before testing, we believe larger depths would result in more precise predictions, but too high of a depth would result in overfitting. The values we tested for max leaf nodes were 3, 10, 25, 50, 100, 200, and None which was the baseline value. We assumed that having no max number of leaf nodes would result in better predictions. We tested values of 0.01, 0.05, 0.1, 0.25, and 0.5 for our epsilon, with 0.01 as the baseline. We hypothesized that an epsilon value of 0.05 would result in a good balance of random actions for exploration and optimal actions for learning. 

Our Monte Carlo Search Tree agent didn’t use Ray Rlib and SKLearn like our previous agents. However it did follow a similar pattern of testing parameters where one agent would have one isolated parameter change, and the three other agents would have default parameters. It should be noted that while these three agents have the same parameters they are acting independently and not together against the one agent with the parameter change. Because of how consistent Monte Carlo Search is, we tested with 20 epochs. Each epoch is a full Tron game and the board state would take a step forward until the game terminated. With each step forward, agents would either choose a random action based on epsilon or the action that would result in the highest score. These scores were calculated by searching through all possible states upto a certain depth and returning a score of -50 if it died, 50 if it won, or its length on the board state multiplied by the reward it received.
  
The parameters we decided to change for our Monte Carlo Search Tree are the search depth and epsilon value for greedy exploration. The values we chose for search depth were 1, 2, 3, 4, and 5 with 2 being the default. A disadvantage of a bigger depth is its long run time however its advantage is getting a better grasp of good moves to do. The epsilon values chosen as well as our hypothesis of it are the same as our previous functional approximators.
  
In order to have the four agents play against each other, the code used was based off of the neural network approximation code. Like the original neural network code, the neural network agent is the player indexed at position 0. The choosing of moves for the neural network, training, and running the game is also the same as it was in original neural network code. In contrast to the original neural network code the other three players do not utilize neural networks to choose their moves. Instead, the other three agents exist as objects inside the neural network, and their moves are chosen using the method for choosing moves in the other three algorithms. In the code, the Monte Carlo agent is mapped to position 1, the Random Forest agent to position 2, and the Ensemble agent to position 3. To allow for the three algorithms to choose their moves, modified versions of their classes were reimplemented with methods for running the game removed, leaving only code related to choosing moves based on their respective criterion. In the case of the Ensemble and Random Forest agents, their training is also retained and the data required for training is passed to them from the Neural Network. Furthermore, the training is still done at the end of each game. For choosing moves for the three other agents, the game state is passed to them like it was in their original implementations. Finally, all four agents retain the ability for their parameters to be changed. The reasoning behind this approach of using the Neural Network code as a base is that it is the most complicated of the four agents in terms of code. In addition, it was easy to change the code for choosing moves in the Neural Network agent to use the move choosing methods of the other three agents. Another factor was that the Neural Network was not implemented using the same base code as the other three, so the process of integrating the other three agents was generally the same for all three, whereas integrating the Neural Network into a different algorithm could have been a more difficult and unique task. Furthermore, we collected data on the cumulative reward of each agent over the 100 epochs, so we could see how each agent would perform relative to each other as they learned. We also collected data on the winner of each game, so we could see if any of the agents would start to experience a distinct increase in their win rate.
  
Data collection for testing parameters followed two distinct approaches, one is how we chose to test parameters, and the other is the two types of data we collected. For testing parameters, we chose distinct parameter values and only tested one parameter at a time, meaning when testing a parameter for an agent the other parameters would remain at their baseline values. A key advantage of this approach is it allows for us to see the specific effects that one parameter may have on the model as its value is adjusted. However, we lose out on the potential for finding the optimal combination of parameters for the four agents, as the optimal parameters for an agent could have been composed of parameter values that were not the baseline values. Our primary reasoning for not testing different combinations of parameter values was that certain models already took a significant amount of time to run through all of the individual parameter values, leaving us without sufficient time to test combinations of parameter values. A second disadvantage to our approach is we are only able to get an approximation of the optimal value for a parameter since we’re only testing a set of values and not a complete range. Again, the reason for this decision is the extreme runtime necessary to run through a range of values and that it isn’t needed to show that a parameter has an effect on the functional approximator.
 
The two types of data values we collected at the end of each epoch were the normalized cumulative reward of the modified agent and the difference in rewards of the modified agent and the averaged rewards of the three opponent agents. In normalizing the cumulative reward, we would remove the reward for winning a game if the observed agent had won, with the goal being that we wanted to have more consistent data from non-winning games. We chose to record data on the normalized cumulative reward since the ColosseumRL Tron game rewarded an agent with a reward of 1 for just surviving, so a higher cumulative reward indicated the agent was surviving longer by learning. In addition, when possible we trained the agents to maximize the cumulative received reward from ColossuemRL, so it made sense to record the data for the values the agents were trying to predict. The second type of data collected was the difference in reward between the observed agent’s final cumulative reward and the averaged final cumulative reward of the other agents, which we refer to as the delta reward. An issue for our recorded delta reward was that games where the observed agent won were normalized by removing the bonus reward for winning, but this normalization was not applied to games where other agents won. As a result, the negative delta rewards are greater in magnitude than expected, but our analysis accounts for this error. Our goal in measuring delta reward was to see how the modified agent performed relative to the unmodified agents. A positive delta reward indicated that the modified agent was outperforming the unmodified agents. We hoped to see a consistently positive or negative delta reward, as it would clearly indicate whether a parameter had an effect on the model. Another potential metric we could have measured was the win rates of modified agents over the epochs, but it was a metric that was disconnected from the reward values and could already be encompassed by the delta reward.

## Evaluation:

## References:
